{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 01-2 张量的基本操作\n",
    "tensor的基本操作：\n",
    "- 拼接与切分\n",
    "- 索引\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import torch\n",
    "torch.manual_seed(1)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f290c0848d0>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ]
  },
  {
   "source": [
    "torch.cat\n",
    "\n",
    "将张量按维度dim进行拼接"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t:tensor([[1., 1., 1.],\n        [1., 1., 1.]]) shape:torch.Size([2, 3])\nt_0:tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]]) shape:torch.Size([4, 3])\nt_1:tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1.]]) shape:torch.Size([2, 9])\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones((2, 3))\n",
    "\n",
    "print(\"t:{} shape:{}\".format(t, t.shape))\n",
    "\n",
    "t_0 = torch.cat([t, t], dim=0)\n",
    "t_1 = torch.cat([t, t, t], dim=1)\n",
    "\n",
    "print(\"t_0:{} shape:{}\\nt_1:{} shape:{}\".format(t_0, t_0.shape, t_1, t_1.shape))"
   ]
  },
  {
   "source": [
    "torch.stack\n",
    "\n",
    "在新创建的维度dim上进行拼接"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nt_stack:tensor([[[1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.]]]) shape:torch.Size([3, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones((2, 3))\n",
    "\n",
    "t_stack = torch.stack([t, t, t], dim=0)\n",
    "\n",
    "print(\"\\nt_stack:{} shape:{}\".format(t_stack, t_stack.shape))"
   ]
  },
  {
   "source": [
    "torch.chunk\n",
    "\n",
    "- 将张量按维度dim进行平均切分\n",
    "- 注意事项：若不能整除，最后一份张量小于其他张量"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "第1个张量：tensor([[1., 1., 1.],\n        [1., 1., 1.]]), shape is torch.Size([2, 3])\n第2个张量：tensor([[1., 1., 1.],\n        [1., 1., 1.]]), shape is torch.Size([2, 3])\n第3个张量：tensor([[1.],\n        [1.]]), shape is torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 7))  # 7\n",
    "list_of_tensors = torch.chunk(a, dim=1, chunks=3)   # 3\n",
    "\n",
    "for idx, t in enumerate(list_of_tensors):\n",
    "    print(\"第{}个张量：{}, shape is {}\".format(idx+1, t, t.shape))"
   ]
  },
  {
   "source": [
    "torch.split\n",
    "\n",
    "将张量按维度dim进行切分\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "第0个张量：tensor([[1., 1.],\n        [1., 1.]]), shape is torch.Size([2, 2])\n第1个张量：tensor([[1.],\n        [1.]]), shape is torch.Size([2, 1])\n第2个张量：tensor([[1., 1.],\n        [1., 1.]]), shape is torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones((2, 5))\n",
    "\n",
    "list_of_tensors = torch.split(t, [2, 1, 2], dim=1)\n",
    "for idx, t in enumerate(list_of_tensors):\n",
    "    print(\"第{}个张量：{}, shape is {}\".format(idx, t, t.shape))"
   ]
  },
  {
   "source": [
    "torch.index_select\n",
    "\n",
    "在维度dim上，按index索引数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t:\ntensor([[4, 5, 0],\n        [5, 7, 1],\n        [2, 5, 8]])\nt_select:\ntensor([[4, 5, 0],\n        [2, 5, 8]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.randint(0, 9, size=(3, 3))\n",
    "idx = torch.tensor([0, 2], dtype=torch.long)    # float\n",
    "t_select = torch.index_select(t, dim=0, index=idx)\n",
    "print(\"t:\\n{}\\nt_select:\\n{}\".format(t, t_select))"
   ]
  },
  {
   "source": [
    "torch.masked_select\n",
    "\n",
    "按mask中的True进行索引\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t:\ntensor([[0, 2, 3],\n        [1, 8, 4],\n        [0, 3, 6]])\nmask:\ntensor([[ True,  True,  True],\n        [ True, False,  True],\n        [ True,  True, False]])\nt_select:\ntensor([0, 2, 3, 1, 4, 0, 3]) \n"
     ]
    }
   ],
   "source": [
    "t = torch.randint(0, 9, size=(3, 3))\n",
    "mask = t.le(5)  # ge is mean greater than or equal/   gt: greater than  le  lt\n",
    "t_select = torch.masked_select(t, mask)\n",
    "print(\"t:\\n{}\\nmask:\\n{}\\nt_select:\\n{} \".format(t, mask, t_select))"
   ]
  },
  {
   "source": [
    "torch.reshape\n",
    "\n",
    "变换张量形状,当张量在内存中是连续时，新张量与input共享数据内存"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t:tensor([2, 0, 1, 6, 3, 4, 7, 5])\nt_reshape:\ntensor([[[2, 0],\n         [1, 6]],\n\n        [[3, 4],\n         [7, 5]]])\nt:tensor([1024,    0,    1,    6,    3,    4,    7,    5])\nt_reshape:\ntensor([[[1024,    0],\n         [   1,    6]],\n\n        [[   3,    4],\n         [   7,    5]]])\nt.data 内存地址:139812884680784\nt_reshape.data 内存地址:139812884680784\n"
     ]
    }
   ],
   "source": [
    "t = torch.randperm(8)\n",
    "t_reshape = torch.reshape(t, (-1, 2, 2))    # -1\n",
    "print(\"t:{}\\nt_reshape:\\n{}\".format(t, t_reshape))\n",
    "\n",
    "t[0] = 1024\n",
    "print(\"t:{}\\nt_reshape:\\n{}\".format(t, t_reshape))\n",
    "print(\"t.data 内存地址:{}\".format(id(t.data)))\n",
    "print(\"t_reshape.data 内存地址:{}\".format(id(t_reshape.data)))"
   ]
  },
  {
   "source": [
    "torch.transpose\n",
    "\n",
    "交换张量的两个维度"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t shape:torch.Size([2, 3, 4])\nt_transpose shape: torch.Size([2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand((2, 3, 4))\n",
    "t_transpose = torch.transpose(t, dim0=1, dim1=2)    # c*h*w     h*w*c\n",
    "print(\"t shape:{}\\nt_transpose shape: {}\".format(t.shape, t_transpose.shape))"
   ]
  },
  {
   "source": [
    "torch.squeeze\n",
    "- 压缩长度为1的维度(轴)\n",
    "- dim: 若为None，移除所有长度为1的轴；若指定维度，当且仅当该轴长度为1时，可以被移除"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 2, 3, 1])\ntorch.Size([2, 3])\ntorch.Size([2, 3, 1])\ntorch.Size([1, 2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand((1, 2, 3, 1))\n",
    "t_sq = torch.squeeze(t)\n",
    "t_0 = torch.squeeze(t, dim=0)\n",
    "t_1 = torch.squeeze(t, dim=1)\n",
    "print(t.shape)\n",
    "print(t_sq.shape)\n",
    "print(t_0.shape)\n",
    "print(t_1.shape)\n"
   ]
  },
  {
   "source": [
    "torch.add\n",
    "\n",
    "逐元素计算 input+alpha×other\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t_0:\ntensor([[ 0.2424,  0.8616,  0.0727],\n        [ 1.3484, -0.8737, -0.2693],\n        [-0.5124, -0.2997,  0.6655]])\nt_1:\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])\nt_add_10:\ntensor([[10.2424, 10.8616, 10.0727],\n        [11.3484,  9.1263,  9.7307],\n        [ 9.4876,  9.7003, 10.6655]])\n"
     ]
    }
   ],
   "source": [
    "t_0 = torch.randn((3, 3))\n",
    "t_1 = torch.ones_like(t_0)\n",
    "t_add = torch.add(t_0, 10, t_1)\n",
    "\n",
    "print(\"t_0:\\n{}\\nt_1:\\n{}\\nt_add_10:\\n{}\".format(t_0, t_1, t_add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}